笔记
==============
**这篇文章似乎是on-policy的，好像和我们的重点并不一致？**
提示：可以善用Pycharm的Crtl+Shift+F 在整个路径中搜索的功能
核心函数：rl_exp_cv.py:26 main()  
  
几个重要的东西：  
Policy：tf_policies.py:325 tfGaussianMLPPolicy。应该是一个参数化的policy？基于Tensorflow写的，我们之后要把它改成pytorch  
ae(AdvantageEstimator)：不太确定这是估计什么的，advantage_estimator.py:11 AdvantageEstimator。  
oracle：“优化问题的目标函数的表示”，reinforcement_oracles.py:111 tfPolicyGradientSimpleCV。  
以上三个东西被用作构造Algorithm时传入的参数
  
algorithm：parser.py:150 create_cv_algorithm 里面是构建了基础算法RobustAdaptiveSecondOrderUpdate，在套上一层BasicOnlineOptimizer，最后用SimpleCVRL包裹得到最终的算法SimpleCVRL。
可以关注一下抽象类Algorithm(algorithm.py:4)，我们要实现的应该就是个类似于它的东西
进行实验（训练）：experimenter.py:41-51
```python
        # Main loop
        for itr in range(n_itrs):
            logz.log_tabular("Time", time.time() - start_time)
            logz.log_tabular("Iteration", itr)
            with timed('Generate env rollouts'):
                ro = self.gen_ro(to_log=True)
            # algorithm-specific
            self._alg.update(ro, gen_env_ro=self._gen_ro)
            logz.dump_tabular()  # dump log
            if save_policy and isinstance(save_freq, int) and itr % save_freq == 0:
                save_policy_fun('{}'.format(itr))
```
里面，`ro = self.gen_ro(to_log=True)`是在产生rollout，也就是利用target policy操作游戏，产生状态和动作的序列（称为rollout），
`self._alg.update(ro, gen_env_ro=self._gen_ro)`中`self._alg`是Algorithm（SimpleCVRL）的对象，以所有的rollout为参数调用其update方法，这应该就是把模型中的参数更新了（学习过程）

## 
一次update的过程
```python
    def _update(self, env_ro, gen_env_ro):
        # gen_env_ro is just used for computing gradient std.
        assert gen_env_ro is not None

        # XXX If using simulation to train vf, vf should be updated after policy nor is updated.
        if self.gen_sim_ro is not None:
            with timed('Generate sim data'):
                sim_ro = self.gen_sim_ro()
            with timed('Update ae'):
                self._or.update_ae(sim_ro, to_log=True)  # update value function

        if self.log_sigmas_freq is not None and self._itr % self.log_sigmas_freq == 0:
            with timed('Compute Sigmas'):
                self._or.log_sigmas(**self.log_sigmas_kwargs)

        with timed('Update Oracle'):
            self._or.update(env_ro, update_nor=True, to_log=True, itr=self._itr)
        with timed('Compute Grad'):
            grads = self._or.compute_grad(ret_comps=True)
            grad = grads[0]
            names = ['g', 'mc_g', 'ac_os', 'tau_os']
            for g, name in zip(grads, names):
                logz.log_tabular('norm_{}'.format(name), la.norm(g))
        with timed('Take Gradient Step'):
            self._learner.update(grad, self._or.ro)  # take the grad with the env_ro
        if self.gen_sim_ro is None:
            with timed('Update ae'):
                self._or.update_ae(env_ro, to_log=True)  # update value function
        # Always update dynamics using true data.
        with timed('Update dyn'):
            self._or.update_dyn(env_ro, to_log=True)  # update dynamics
        with timed('Update rw'):
            self._or.update_rw(env_ro, to_log=True)
        self._itr += 1
        logz.log_tabular('online_learner_stepsize', self._learner.stepsize)
        logz.log_tabular('std', np.mean(self._policy.std))
```


## 
oracle.py、policy.py、advantage_estimator.py中有对它是什么的解释  
```
class Oracle(ABC):
    """
        An Oracle defines the objective function for an optimization problem.

        'compute_loss' and 'compute_grad' provide the interface for the
        optimization routine. 'update' redefines the objective function.

        A child class needs to support __deepcopy__.
    """
```

```
class Policy(FunctionApproximator):  # a policy is namely a stochastic FunctionApproximator
    """
    An abstract interface that represents conditional distribution \pi(a|s).

    It should be deepcopy compatible.
    """
```

```
class AdvantageEstimator(object):
    # An estimator based on value function
```